{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "To build a 3D Conv model that will be able to predict the 5 gestures correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='cyan'> Sections in this notebook: </font>\n",
    "I. Prerequisites\n",
    "    \n",
    "    I.1. Importing all the necessary modules\n",
    "    \n",
    "II. Custom Generators\n",
    "\n",
    "    II.1. Abstract base class: Generator\n",
    "    II.2. Approach 1\n",
    "    II.3. Approach 2\n",
    "    II.4. Approach 3\n",
    "\n",
    "III. Model Deployment\n",
    "    \n",
    "    III.1. ModelDeployment Class\n",
    "    III.2. Let's try various models \n",
    "        III.2.1. Inception + RNN\n",
    "        III.2.2. Custom Conv3d\n",
    "        III.2.3. Optical flow followed by a custom conv3d network\n",
    "        III.2.4. Quo Vadis?\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='goldenrod'> I. Prerequisites </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='skyblue'>  I.1. Importing all the necessary modules </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os\n",
    "from math import ceil\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"asparagus\"> $\\Rightarrow$ Just making sure GPU is getting used</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='goldenrod'> II. Custom Generators </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"asparagus\"> $\\Rightarrow$ A little heads up about the design used...</font> <br>\n",
    "- We are going to try different approaches to tackle this problem. \n",
    "- All the approaches will require the data to be in a certain way. Therefore, we've opted to create an abstract base class called `Generator`.\n",
    "- Any generator we write will inherit from this class and they will implement the mandatory virtual functions for them to work.\n",
    "\n",
    "- As for the working of each method, we have written comments before each method's definition in the abstract class itself. <br> \n",
    "- Some generators need additional methods for them to work, in which case, comments will be present before their definition in the children classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='skyblue'>  II.1. Abstract base class: Generator </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(keras.utils.Sequence):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Main logic to get one batch of data is implemented in this method.\n",
    "     @returns: (batch_data, batch_labels). \n",
    "     - The shape of batch_data depends on the type of generator getting used. \n",
    "     Regardless, the first dimension will be batch_size\n",
    "     - batch_labels.shape = (batch_size, numClasses)\n",
    "    \"\"\"\n",
    "    def getBatchData(self, batch, currBatchSize):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Since all variables are public, this is not really needed. But keeping it anyway.\n",
    "    # @return: Number of batches to run: depends on total number of videos/batch size \n",
    "    \"\"\"\n",
    "    def getNumBatches(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" \n",
    "    - Since we are inheriting from keras.utils.Sequence, we need to implement __getitem__ & __len__.\n",
    "    - model.fit will call __getitem__ to get one batch of data. \n",
    "    - In our implementation, this acts like a wrapper around getBatchData\n",
    "    \"\"\"\n",
    "    def __getitem__(self, batchIdx):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Returns number of batches \"\"\"\n",
    "    def __len__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='skyblue'>  II.2. Approach 1 </font> \n",
    "<font color=\"asparagus\"> $\\Rightarrow$ We use a pretrained CNN model like inception-v3 to extract features & then feed them to RNN to predict the class </font> <br>\n",
    "<font color=\"asparagus\"> $\\Rightarrow$ This is the generator for this purpose: CNNRNNGenerator </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNRNNGenerator(Generator):\n",
    "    \"\"\"\n",
    "      Parameters' description:\n",
    "    - frameIdxList:    Choice left to the user, if user wants to utilize all frames: it'll be list(range(1,30)), since each frame has 30 frames\n",
    "    - (width, height): Generator resized each frame to (width, height). For inception-v3, the input must of shape (224, 224, 3)\n",
    "    - source_path:     Path to data(train/val)\n",
    "    - batch_size:      self-explanatory\n",
    "    - dataCSV:         <PathToCsv>/<NameOfCSV>.csv\n",
    "    - numClasses:      For our data, the number of actions = 5: swipe(left to right), swipe(right to left), stop, thumps up, thumps down\n",
    "    - numFeatures:     Output shape as per feature extractor, in our case, inception-v3, which returns 2048.\n",
    "    Constructor just caches all the important member variables. Did not want to complicate things with access modifiers, keeping everything public : )\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 frameIdxList,\n",
    "                 width=224,\n",
    "                 height=224,\n",
    "                 source_path=r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\train\",\n",
    "                 batch_size=30,\n",
    "                 dataCSV=r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\train.csv\",\n",
    "                 numClasses=5,\n",
    "                 numFeatures=2048,  # as per inception-v3\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        # Shuffle the data and store in a list\n",
    "        self.frameIdxList = frameIdxList\n",
    "        self.numFramesInVideo = len(frameIdxList)\n",
    "        self.batch_size = batch_size\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.source_path = source_path\n",
    "        self.dataCSV = dataCSV\n",
    "        self.numClasses = numClasses\n",
    "        self.numFeatures = numFeatures\n",
    "\n",
    "        self.data_doc = np.random.permutation(open(self.dataCSV).readlines())\n",
    "\n",
    "        # Get vector list\n",
    "        self.vectorList = np.random.permutation(self.data_doc)\n",
    "        self.numVideos = len(self.vectorList)\n",
    "        self.remBatchSize = self.numVideos % self.batch_size\n",
    "        self.numBatches = ceil(self.numVideos / self.batch_size)\n",
    "        self.currBatchIdx = 0\n",
    "        self.numChannels = 3\n",
    "        self.featureExtractor = None\n",
    "\n",
    "    \"\"\"\n",
    "    Build the feature extractor(inception-v3 in our case) and return it.\n",
    "    \"\"\"\n",
    "    def __build_feature_extractor(self, preTrainedModel=\"inceptionv3\"):\n",
    "        featureExtractor = None\n",
    "        if preTrainedModel == \"inceptionv3\":\n",
    "            # Disable output layer, so when we do model.predict, it only returns the feature map and the class\n",
    "            # probabilities. I think imagenet has 1000 classes, we don't need their probabilities.\n",
    "            featureExtractor = keras.applications.InceptionV3(\n",
    "                weights=\"imagenet\",\n",
    "                include_top=False,\n",
    "                pooling=\"avg\",\n",
    "                input_shape=(self.width, self.height, self.numChannels))\n",
    "            # Get preprocess_input from inception_v3 module.\n",
    "            # This is important because it is responsible for normalizing and converting the image to the format with\n",
    "            # which inception_v3 was trained on.\n",
    "            preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "            inputs = keras.Input((self.width, self.height, self.numChannels))\n",
    "            preprocessed = preprocess_input(inputs)\n",
    "            outputs = featureExtractor(preprocessed)\n",
    "            return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "        # Try other feature extractors, if needed\n",
    "        elif preTrainedModel == \"vgg16\":\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \"\"\"\n",
    "    Redundant as numBatches is public, but leaving it as is.\n",
    "    \"\"\"\n",
    "    def getNumBatches(self):\n",
    "        return self.numBatches\n",
    "\n",
    "    \"\"\" \n",
    "    Since we are using inception-v3 to extract features, we'll initialize it with this method\n",
    "    \"\"\"\n",
    "    def initializeFeatureExtractor(self, preTrainedModel=\"inceptionv3\"):\n",
    "        self.featureExtractor = self.__build_feature_extractor(preTrainedModel)\n",
    "\n",
    "    \"\"\"\n",
    "    As described in Generator class, this returns one batch worth of data which is a tuple of \n",
    "    batch_data, whose shape is (batch_size, numFramesInVideo, numFeatures) and\n",
    "    batch_labels, whose shape is (batch_size, numFramesInVideo, numClasses)\n",
    "    \"\"\"\n",
    "    def getBatchData(self, batch, currBatchSize):\n",
    "        # The input to RNN will be the output of CNN(inceptionv3)\n",
    "        # The output of inceptionv3 will be 2048. Since we have 30 frames(numFramesInVideo), batch_data: 30, 2048.\n",
    "        # Finally, every batch will have this input.\n",
    "        batch_data = np.zeros((self.batch_size, self.numFramesInVideo, self.numFeatures))\n",
    "        batch_labels = np.zeros((self.batch_size, self.numFramesInVideo, self.numClasses))\n",
    "        for batchIdx in range(currBatchSize):\n",
    "            # Get the vector name (or directory name)\n",
    "            unprocessedCurrVectorString = self.vectorList[batchIdx + (batch * self.batch_size)].strip()\n",
    "            vectorName = unprocessedCurrVectorString.split(\";\")[0]\n",
    "            # Store vector path\n",
    "            vectorDir = self.source_path + \"\\\\\" + vectorName\n",
    "\n",
    "            # List all the frames within the directory(vectorName). # Read each image one by one\n",
    "            allFrames = os.listdir(vectorDir)\n",
    "\n",
    "            # videoData has all the images imread\n",
    "            videoData = np.zeros((self.numFramesInVideo, self.width, self.height, self.numChannels))\n",
    "\n",
    "            # Its corresponding label\n",
    "            videoLabel = np.zeros((self.numFramesInVideo, self.numClasses))\n",
    "\n",
    "            # Current video's frames loop\n",
    "            for frameIdx in self.frameIdxList:\n",
    "                # Get path of current frame using frameIdx\n",
    "                currentFrame = vectorDir + \"\\\\\" + allFrames[frameIdx]\n",
    "                # Read image\n",
    "                frame = cv2.imread(currentFrame).astype(np.float32)\n",
    "                # Resize the image to what the feature extractor wants\n",
    "                resizedCurrentFrame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "                # Store frame in videoData tensor.\n",
    "                videoData[frameIdx,] = resizedCurrentFrame\n",
    "                # Set the corresponding class' index to 1: One hot encoding\n",
    "                videoLabel[frameIdx, int(unprocessedCurrVectorString.split(\";\")[2])] = 1\n",
    "\n",
    "            batch_data[batchIdx,] = self.featureExtractor.predict(videoData)\n",
    "            batch_labels[batchIdx,] = videoLabel\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Return number of batches in dataset. Needed as we are inheriting from keras.utils.Sequence\n",
    "    \"\"\"\n",
    "    def __len__(self):\n",
    "        return self.numBatches\n",
    "    \n",
    "    \"\"\"\n",
    "    - Needed as we are inheriting from keras.utils.Sequence, model.fit calls __getitem__\n",
    "    - __getitem__ is a wrapper around getBatchData\n",
    "    \"\"\"\n",
    "    def __getitem__(self, batchIdx):\n",
    "        # Adjusting batch size as per batchIdx. Remember the last batch may not have enough data to be equal to\n",
    "        # numVideos/batch_size, it will be equal to numVideos % batch_size.\n",
    "        currBatchSize = self.batch_size if batchIdx < (self.numVideos // self.batch_size) else self.remBatchSize\n",
    "        batch_data, batch_labels = self.getBatchData(batchIdx, currBatchSize)\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "    \"\"\" Keeping this for debugging purposes. Not really needed for the functioning of our code.\n",
    "        We used it to test generator with next(generator).\n",
    "    \"\"\"\n",
    "    def __next__(self):\n",
    "        batch_data = None\n",
    "        batch_labels = None\n",
    "        # The following logic is to make sure when data is exhausted, the generator starts over from batchIdx = 0\n",
    "        if self.currBatchIdx < self.__len__():\n",
    "            batch_data, batch_labels = self.__getitem__(self.currBatchIdx)\n",
    "            self.currBatchIdx += 1\n",
    "        else:\n",
    "            self.currBatchIdx = 0\n",
    "            batch_data, batch_labels = self.__getitem__(self.currBatchIdx)\n",
    "        return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='skyblue'>  II.3. Approach 2 </font> \n",
    "<font color=\"asparagus\"> $\\Rightarrow$ This is probably the simplest generator. <br> <font color=\"asparagus\"> $\\Rightarrow$ Here we create a custom Conv3D network, therefore the input to this network is the images itself. </font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3DGenerator(Generator):\n",
    "    \"\"\"\n",
    "      Parameters' description:\n",
    "    - frameIdxList:    Choice left to the user, if user wants to utilize all frames: it'll be list(range(1,30)), since each frame has 30 frames\n",
    "    - (width, height): Generator resized each frame to (width, height). For inception-v3, the input must of shape (224, 224, 3)\n",
    "    - source_path:     Path to data(train/val)\n",
    "    - batch_size:      self-explanatory\n",
    "    - dataCSV:         <PathToCsv>/<NameOfCSV>.csv\n",
    "    - numClasses:      For our data, the number of actions = 5: swipe(left to right), swipe(right to left), stop, thumps up, thumps down\n",
    "    Constructor just caches all the important member variables. Did not want to complicate things with access modifiers, keeping everything public : )\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 frameIdxList,\n",
    "                 width=224,\n",
    "                 height=224,\n",
    "                 source_path=r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\train\",\n",
    "                 batch_size=30,\n",
    "                 dataCSV=r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\train.csv\",\n",
    "                 numClasses=5):\n",
    "        super().__init__()\n",
    "        # Shuffle the data and store in a list\n",
    "        self.frameIdxList = frameIdxList\n",
    "        self.numFramesInVideo = len(frameIdxList)\n",
    "        self.batch_size = batch_size\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        self.source_path = source_path\n",
    "        self.dataCSV = dataCSV\n",
    "        self.numClasses = numClasses\n",
    "        self.data_doc = np.random.permutation(open(self.dataCSV).readlines())\n",
    "\n",
    "        # Get vector list\n",
    "        self.vectorList = np.random.permutation(self.data_doc)\n",
    "        self.numVideos = len(self.vectorList)\n",
    "        self.remBatchSize = self.numVideos % self.batch_size\n",
    "        self.numBatches = ceil(self.numVideos / self.batch_size)\n",
    "        self.currBatchIdx = 0\n",
    "        self.numChannels = 3\n",
    "    \n",
    "    \"\"\"\n",
    "    - Redundant as numBatches is public, but leaving it as is.\n",
    "    \"\"\"\n",
    "    def getNumBatches(self):\n",
    "        return self.numBatches\n",
    "    \n",
    "    \"\"\"\n",
    "    - Return number of batches in dataset. Needed as we are inheriting from keras.utils.Sequence\n",
    "    \"\"\"\n",
    "    def __len__(self):\n",
    "        return self.numBatches\n",
    "    \n",
    "    \"\"\"\n",
    "    - As described in Generator class, this returns one batch worth of data which is a tuple of \n",
    "    batch_data, whose shape is (batch_size, numFramesInVideo, width, height, numChannels) and\n",
    "    batch_labels, whose shape is (batch_size, numClasses)\n",
    "    \"\"\"\n",
    "    def getBatchData(self, batch, currBatchSize):\n",
    "        # The input to Conv3d will just be the frames of each video\n",
    "        batch_data = np.zeros((self.batch_size, self.numFramesInVideo, self.width, self.height, self.numChannels))\n",
    "        batch_labels = np.zeros((self.batch_size, self.numClasses))\n",
    "        for batchIdx in range(currBatchSize):\n",
    "            # Get the vector name (or directory name)\n",
    "            unprocessedCurrVectorString = self.vectorList[batchIdx + (batch * self.batch_size)].strip()\n",
    "            vectorName = unprocessedCurrVectorString.split(\";\")[0]\n",
    "            # Store vector path\n",
    "            vectorDir = self.source_path + \"\\\\\" + vectorName\n",
    "\n",
    "            # List all the frames within the directory(vectorName). # Read each image one by one\n",
    "            allFrames = os.listdir(vectorDir)\n",
    "\n",
    "            # videoData has all the images imread\n",
    "            videoData = np.zeros((self.numFramesInVideo, self.width, self.height, self.numChannels))\n",
    "\n",
    "            # Its corresponding label\n",
    "            videoLabel = np.zeros((self.numFramesInVideo, self.numClasses))\n",
    "\n",
    "            # Current video's frames loop\n",
    "            for frameIdx in self.frameIdxList:\n",
    "                # Get path of current frame using frameIdx\n",
    "                currentFrame = vectorDir + \"\\\\\" + allFrames[frameIdx]\n",
    "                # Read image\n",
    "                frame = cv2.imread(currentFrame).astype(np.float32)\n",
    "\n",
    "                # Resize the image to (224, 224, 3).\n",
    "                resizedCurrentFrame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                # Normalize\n",
    "                resizedCurrentFrame = resizedCurrentFrame / 255.0\n",
    "                # Store frame in videoData tensor.\n",
    "                videoData[frameIdx,] = resizedCurrentFrame\n",
    "\n",
    "            batch_data[batchIdx,] = videoData\n",
    "            # Set the corresponding class' index to 1: One hot encoding\n",
    "            batch_labels[batchIdx, int(unprocessedCurrVectorString.split(\";\")[2])] = 1\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "    \"\"\"\n",
    "    - Needed as we are inheriting from keras.utils.Sequence, model.fit calls __getitem__()\n",
    "    - __getitem__() is a wrapper around getBatchData\n",
    "    \"\"\"\n",
    "    def __getitem__(self, batchIdx):\n",
    "        # Adjusting batch size as per batchIdx. Remember the last batch may not have enough data to be equal to\n",
    "        # numVideos/batch_size, it will be equal to numVideos % batch_size.\n",
    "        currBatchSize = self.batch_size if batchIdx < (self.numVideos // self.batch_size) else self.remBatchSize\n",
    "        batch_data, batch_labels = self.getBatchData(batchIdx, currBatchSize)\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "    \"\"\" \n",
    "    - Keeping this for debugging purposes. Not really needed for the functioning of our code.\n",
    "    - We used it to test generator with next(generator).\n",
    "    \"\"\"\n",
    "    def __next__(self):\n",
    "        batch_data = None\n",
    "        batch_labels = None\n",
    "        # The following logic is to make sure when data is exhausted, the generator starts over from batchIdx = 0\n",
    "        if self.currBatchIdx < self.__len__():\n",
    "            batch_data, batch_labels = self.__getitem__(self.currBatchIdx)\n",
    "            self.currBatchIdx += 1\n",
    "        else:\n",
    "            self.currBatchIdx = 0\n",
    "            batch_data, batch_labels = self.__getitem__(self.currBatchIdx)\n",
    "        return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='skyblue'>  II.4. Approach 3 </font> \n",
    "<font color=\"asparagus\"> $\\Rightarrow$ Even in this approach, we create a custom Conv3D network like the Conv3DGenerator, but the only difference being the input is not the images. <br> </font>\n",
    "<font color=\"asparagus\"> $\\Rightarrow$ Rather, we obtain a dense optical flow output between consecutive frames and feed that to a Conv3D network to predict the classes. <br>"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"asparagus\"> $\\Rightarrow$ `A bit about Optical Flow` <br>\n",
    "- Optical flow is known as the pattern of apparent motion of objects, i.e, it is the motion of objects between every two consecutive frames of the sequence, which is caused by the movement of the object being captured or the camera capturing it. Consider an object with intensity I (x, y, t), after time dt, it moves to by dx and dy, now, the new intensity would be, I (x+dx, y+dy, t+dt).\n",
    "- We, assume that the pixel intensities are constant between the two frames, i.e., \n",
    "I (x, y, t) = I (x+dx, y+dy, t+dt) \n",
    "Taylor approximation is done on the RHS side, resulting in: \n",
    "![image-2.png](attachment:image-2.png)\n",
    "- On dividing by δt, we obtain the Optical Flow Equation, i.e., \n",
    "    ![image.png](attachment:image.png)\n",
    "where, u = dx/dt and v = dy/dt. \n",
    "- Also, dI/dx is the image gradient along the horizontal axis, dI/dy is the image gradient along the vertical axis and dI/dt is along the time. \n",
    "Since, we have just one equation to find two unknowns, we use different approaches to solve this:\n",
    "- openCV has Lucas-Kanade, which first obtains features like corners using Corner detectors like Shi-Tomasi or Harris Corner detection and obtains the flow vectors for those points only. Therefore, this is termed as sparse optical flow. \n",
    "- We wanted the input shape to be fixed and not variable. Therefore, we went with dense optical flow. \n",
    "- In dense optical flow, the flow vectors are obtained for each pixel in the frame. So in our case where the image is (224, 224), the output of dense optical flow will be (224,224,2). The '2' captures change in x and y: (dx/dt, dy/dt)\n",
    "- OpenCV has Gunnar-Farnebäck implemented, which is what we will use.\n",
    "- FYI, there is a better approach to get dense OF which utilizes SGM(semi global-matching), I leave it up to you read [this paper](https://core.ac.uk/download/pdf/11134866.pdf) for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpticalFlowGenerator(Generator):\n",
    "    \"\"\"\n",
    "    - The constructor differs slighly from the generators' constructor: \n",
    "    - We can use an existing vector list(which has already been shuffled). In such scenarios, we provide two more arguments: useVectorList(a boolean) and the actual vectorList to be used.\n",
    "    - This was implemented because we wanted to try an approach suggested by this paper: https://arxiv.org/abs/1705.07750\n",
    "      Parameters' description:\n",
    "    - frameIdxList:    Choice left to the user, if user wants to utilize all frames: it'll be list(range(1,30)), since each frame has 30 frames\n",
    "    - (width, height): Generator resized each frame to (width, height). For inception-v3, the input must of shape (224, 224, 3)\n",
    "    - source_path:     Path to data(train/val)\n",
    "    - batch_size:      self-explanatory\n",
    "    - dataCSV:         <PathToCsv>/<NameOfCSV>.csv\n",
    "    - numClasses:      For our data, the number of actions = 5: swipe(left to right), swipe(right to left), stop, thumps up, thumps down\n",
    "    - useVectorList:   A boolean to indicate whether we want to use an existing shuffled vector list\n",
    "    - vectorList:      The vector list to be used.\n",
    "    Constructor just caches all the important member variables. Did not want to complicate things with access modifiers, keeping everything public : )\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 frameIdxList,\n",
    "                 width=224,\n",
    "                 height=224,\n",
    "                 source_path=r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\train\",\n",
    "                 batch_size=30,\n",
    "                 dataCSV=r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\train.csv\",\n",
    "                 numClasses=5,\n",
    "                 useVectorList=False,\n",
    "                 vectorList=None):\n",
    "        super().__init__()\n",
    "        # Shuffle the data and store in a list\n",
    "        self.frameIdxList = frameIdxList\n",
    "        self.numFramesInVideo = len(frameIdxList)\n",
    "        self.batch_size = batch_size\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        self.source_path = source_path\n",
    "        self.dataCSV = dataCSV\n",
    "        self.numClasses = numClasses\n",
    "        self.data_doc = np.random.permutation(open(self.dataCSV).readlines())\n",
    "\n",
    "        # Get vector list\n",
    "        if useVectorList:\n",
    "            self.vectorList = vectorList\n",
    "        else:\n",
    "            self.vectorList = np.random.permutation(self.data_doc)\n",
    "        self.numVideos = len(self.vectorList)\n",
    "        self.remBatchSize = self.numVideos % self.batch_size\n",
    "        self.numBatches = ceil(self.numVideos / self.batch_size)\n",
    "        self.currBatchIdx = 0\n",
    "        self.numChannels = 3\n",
    "        \n",
    "    \"\"\"\n",
    "    - Redundant as numBatches is public, but leaving it as is.\n",
    "    \"\"\"\n",
    "    def getNumBatches(self):\n",
    "        return self.numBatches\n",
    "\n",
    "    \"\"\"\n",
    "    - Return number of batches in dataset. Needed as we are inheriting from keras.utils.Sequence\n",
    "    \"\"\"\n",
    "    def __len__(self):\n",
    "        return self.numBatches\n",
    "\n",
    "    \"\"\"\n",
    "    An extra method needed for this generator, it does the following:\n",
    "    - Take current and next frame's absolute paths as input arguments.\n",
    "    - imread the two frames\n",
    "    - resize to desired size\n",
    "    - convert to gray(since OF only needs luma component)\n",
    "    - Get dense OF output using Gunnar Farneback's method\n",
    "    - Return OF output\n",
    "    \"\"\"\n",
    "    def getOFOutput(self, currFramePath, nextFramePath):\n",
    "        # Read images\n",
    "        currFrame = cv2.imread(currFramePath)\n",
    "        nextFrame = cv2.imread(nextFramePath)\n",
    "\n",
    "        # Resize to an agreed upon size(Inception wants 224, 224)\n",
    "        resizedCurrentFrame = cv2.resize(currFrame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        resizedNextFrame = cv2.resize(nextFrame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Get the gray versions\n",
    "        currFrameGray = cv2.cvtColor(resizedCurrentFrame, cv2.COLOR_BGR2GRAY)\n",
    "        nextFrameGray = cv2.cvtColor(resizedNextFrame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev=currFrameGray, next=nextFrameGray, flow=None, pyr_scale=0.5,\n",
    "                                            levels=3, winsize=15, iterations=3, poly_n=5, poly_sigma=1.2, flags=0)\n",
    "        return flow\n",
    "    \n",
    "    \"\"\"\n",
    "    - As described in Generator class, this returns one batch worth of data which is a tuple of \n",
    "    batch_data, whose shape is (batch_size, numFramesInVideo - 1, width, height, 2) and\n",
    "    batch_labels, whose shape is (batch_size, numClasses)\n",
    "    Why (numFramesInVideo - 1)? : Because the last frame does not have a target frame i.e. the next frame, so no flow vector for the last frame.\n",
    "    and 2 because as we described earlier, flow vector: (dx/dt, dy/dt)\n",
    "    \"\"\"\n",
    "    def getBatchData(self, batch, currBatchSize):\n",
    "        # The input to Conv3d will be the flow output of each frame\n",
    "        # The last dim is 2 because flow is a vector of dx/dt, dy/dt\n",
    "        batch_data = np.zeros((self.batch_size, self.numFramesInVideo - 1, self.width, self.height, 2))\n",
    "        batch_labels = np.zeros((self.batch_size, self.numClasses))\n",
    "        for batchIdx in range(currBatchSize):\n",
    "            # Get the vector name (or directory name)\n",
    "            unprocessedCurrVectorString = self.vectorList[batchIdx + (batch * self.batch_size)].strip()\n",
    "            vectorName = unprocessedCurrVectorString.split(\";\")[0]\n",
    "            # Store vector path\n",
    "            vectorDir = self.source_path + \"\\\\\" + vectorName\n",
    "\n",
    "            # List all the frames within the directory(vectorName). # Read each image one by one\n",
    "            allFrames = os.listdir(vectorDir)\n",
    "\n",
    "            # videoData has all the images' flow outputs\n",
    "            videoData = np.zeros((self.numFramesInVideo - 1, self.width, self.height,\n",
    "                                  2))  # The last dim is 2 because flow is a vector of dx/dt, dy/dt\n",
    "\n",
    "            # Current video's frames loop\n",
    "            frameIdx = 0\n",
    "            while frameIdx < (\n",
    "                    self.numFramesInVideo - 1):  # Flow will have one less frame since it needs (prev, next) pairs\n",
    "                # Get path of current frame using frameIdx\n",
    "                currentFrame = vectorDir + \"\\\\\" + allFrames[frameIdx]\n",
    "\n",
    "                # Get path of next frame using frameIdx\n",
    "                nextFramePath = vectorDir + \"\\\\\" + allFrames[frameIdx + 1]\n",
    "\n",
    "                flow = self.getOFOutput(currFramePath=currentFrame, nextFramePath=nextFramePath)\n",
    "                videoData[frameIdx,] = flow\n",
    "                frameIdx += 1\n",
    "            batch_data[batchIdx,] = videoData\n",
    "            # Set the corresponding class' index to 1: One hot encoding\n",
    "            batch_labels[batchIdx, int(unprocessedCurrVectorString.split(\";\")[2])] = 1\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "    \"\"\"\n",
    "    - Needed as we are inheriting from keras.utils.Sequence, model.fit calls __getitem__()\n",
    "    - __getitem__() is a wrapper around getBatchData\n",
    "    \"\"\"\n",
    "    def __getitem__(self, batchIdx):\n",
    "        # Adjusting batch size as per batchIdx. Remember the last batch may not have enough data to be equal to\n",
    "        # numVideos/batch_size, it will be equal to numVideos % batch_size.\n",
    "        currBatchSize = self.batch_size if batchIdx < (self.numVideos // self.batch_size) else self.remBatchSize\n",
    "        batch_data, batch_labels = self.getBatchData(batchIdx, currBatchSize)\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "    \"\"\" \n",
    "    - Keeping this for debugging purposes. Not really needed for the functioning of our code.\n",
    "    - We used it to test generator with next(generator).\n",
    "    \"\"\"\n",
    "    def __next__(self):\n",
    "        batch_data = None\n",
    "        batch_labels = None\n",
    "        # The following logic is to make sure when data is exhausted, the generator starts over from batchIdx = 0\n",
    "        if self.currBatchIdx < self.__len__():\n",
    "            batch_data, batch_labels = self.__getitem__(self.currBatchIdx)\n",
    "            self.currBatchIdx += 1\n",
    "        else:\n",
    "            self.currBatchIdx = 0\n",
    "            batch_data, batch_labels = self.__getitem__(self.currBatchIdx)\n",
    "        return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='goldenrod'> III. Model Deployment </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='skyblue'>  III.1. ModelDeployment Class </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"asparagus\"> $\\Rightarrow$ One ModelDeployment class which has methods to get the various models we have tried. </font> <br>\n",
    "<font color=\"asparagus\"> $\\Rightarrow$ The model returned depends modelType argument given during instance creation. </font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDeployment:\n",
    "    \"\"\"\n",
    "    - modelType:               Supported values: cnn-rnn, conv3d, OF or QuoVadis\n",
    "    - numFeaturesInFirstLayer: Valid for when model is a custom conv3d network.\n",
    "    - numFrames:               self explanatory\n",
    "    - input_shape:             A tuple: (width, height, numChannels or 2)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, modelType=\"OF\", numFeaturesInFirstLayer=16, numNeuronsInDenseLayer=128, numFrames=30):\n",
    "        self.modelType = modelType\n",
    "        self.numFeaturesInFirstLayer = numFeaturesInFirstLayer\n",
    "        self.numNeuronsInDenseLayer = numNeuronsInDenseLayer\n",
    "        self.numFrames = 30\n",
    "        self.input_shape = input_shape\n",
    "    \n",
    "    \"\"\"\n",
    "    - We had tried quite a few architectures and this one seems to give better results than the others.\n",
    "    @Returns conv3d model\n",
    "    \"\"\"\n",
    "    def __get_conv3d_model(self):\n",
    "         # Model\n",
    "        model = models.Sequential()\n",
    "        \n",
    "        # Convolution layer with `numFeaturesInFirstLayer` features, 3x3 filter and a relu activation with 2x2 pooling\n",
    "        model.add(layers.Conv3D(self.numFeaturesInFirstLayer, (3, 3, 3), padding='same', activation='relu',\n",
    "                            input_shape=(self.numFrames, self.input_shape[0], self.input_shape[1], self.input_shape[2])))\n",
    "        model.add(layers.MaxPooling3D())\n",
    "        \n",
    "        # Convolution layer with `numFeaturesInFirstLayer * 2` features, 3x3 filter and relu activation with 2x2 pooling\n",
    "        model.add(layers.Conv3D((self.numFeaturesInFirstLayer * 2), (3, 3, 3), padding='same', activation='relu'))\n",
    "        model.add(layers.MaxPooling3D())\n",
    "        \n",
    "        # Convolution layer with `numFeaturesInFirstLayer * 4` features, 3x3 filter and relu activation with 2x2 pooling\n",
    "        model.add(layers.Conv3D((self.numFeaturesInFirstLayer * 4), (3, 3, 3), padding='same', activation='relu'))\n",
    "        model.add(layers.MaxPooling3D())\n",
    "        \n",
    "        # Convolution layer with `numFeaturesInFirstLayer * 8` features, 3x3 filter and relu activation with 2x2 pooling\n",
    "        model.add(layers.Conv3D((self.numFeaturesInFirstLayer * 8), (3, 3, 3), padding='same', activation='relu'))\n",
    "        model.add(layers.MaxPooling3D())\n",
    "        \n",
    "        # Flatten\n",
    "        model.add(layers.Flatten())\n",
    "        \n",
    "        # Add dense layer\n",
    "        model.add(layers.Dense(self.numNeuronsInDenseLayer, activation='relu'))\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        #Output layer\n",
    "        model.add(layers.Dense(5, activation=\"softmax\"))\n",
    "        \n",
    "        # Set optimizer, compile & return model\n",
    "        optimiser = \"adam\"\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        #print(model.summary())\n",
    "        return model\n",
    "        \n",
    "    \"\"\"\n",
    "    - We tried quite a few architectures and this one seems to give better results than the others.\n",
    "    @Returns rnn model that takes input from a pretrained cnn model like inception-v3\n",
    "    \"\"\"\n",
    "    def __get_rnn_model(self, numFrames=30, numFeatures=2048):\n",
    "        frame_features_input = keras.Input(shape=(numFrames, numFeatures))\n",
    "        x = GRU(16, return_sequences=True)(frame_features_input)\n",
    "        # x = GRU(8, return_sequences=True)(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(32, activation=\"relu\")(x)  # Adding one dense layer\n",
    "        dense = Dense(5, activation=\"softmax\")\n",
    "        outputs = TimeDistributed(dense)(x)\n",
    "        rnn_model = keras.Model(inputs=frame_features_input, outputs=outputs)\n",
    "        rnn_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "        return rnn_model\n",
    "    \n",
    "    \"\"\" \n",
    "    - When model is rnn, this method creates a directory to cache the best model and adds ReduceLROnPlateau to the callback list\n",
    "    @Returns callback list\n",
    "    \"\"\"\n",
    "    def __get_rnn_callback(self):\n",
    "        curr_dt_time = datetime.datetime.now()\n",
    "        model_name = r\"rcnn\\rcnn_init\" + '_' + str(curr_dt_time).replace(' ', '').replace(':', '_') + '\\\\'\n",
    "        currDir = os.getcwd()\n",
    "        if not os.path.exists(currDir + \"\\\\\" + \"rcnn\"):\n",
    "            os.mkdir(currDir + \"\\\\\" + \"rcnn\")\n",
    "\n",
    "        modelPath = currDir + \"\\\\\" + model_name\n",
    "        if not os.path.exists(modelPath):\n",
    "            os.mkdir(modelPath)\n",
    "\n",
    "        filepath = modelPath + 'rcnn-{epoch:05d}-{loss:.5f}-{accuracy:.5f}-{val_loss:.5f}-{val_accuracy:.5f}.h5'\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1,\n",
    "                                     save_best_only=True, save_weights_only=False,\n",
    "                                     mode='auto', period=1)\n",
    "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2,\n",
    "                               min_lr=1e-4)  # write the REducelronplateau code here\n",
    "        return [checkpoint, LR]\n",
    "    \n",
    "    \"\"\" \n",
    "    - When model is conv3d or OF, this method creates a directory to cache the best model and adds ReduceLROnPlateau to the callback list\n",
    "    @Returns callback list\n",
    "    \"\"\"    \n",
    "    def __get_conv3d_callback(self):\n",
    "        curr_dt_time = datetime.datetime.now()\n",
    "        model_name = None\n",
    "        if self.modelType == \"conv3d\":\n",
    "            model_name = r\"conv3d\\conv3d_init\" + '_' + str(curr_dt_time).replace(' ', '').replace(':', '_') + '\\\\'\n",
    "            if not os.path.exists(currDir + \"\\\\\" + \"conv3d\"):\n",
    "                os.mkdir(currDir + \"\\\\\" + \"conv3d\")\n",
    "        elif self.modelType == \"OF\":\n",
    "            model_name = r\"conv3d_of\\conv3d_init\" + '_' + str(curr_dt_time).replace(' ', '').replace(':', '_') + '\\\\'\n",
    "            if not os.path.exists(currDir + \"\\\\\" + \"conv3d_of\"):\n",
    "                os.mkdir(currDir + \"\\\\\" + \"conv3d_of\")\n",
    "        \n",
    "        modelPath = currDir + \"\\\\\" + model_name       \n",
    "        if not os.path.exists(modelPath):\n",
    "            os.mkdir(modelPath)\n",
    "        filepath = modelPath + 'conv3d-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1,\n",
    "                                 save_best_only=True, save_weights_only=False,\n",
    "                                 mode='auto', period=1)\n",
    "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2,\n",
    "                           min_lr=1e-4)\n",
    "        return [checkpoint, LR]\n",
    "        \n",
    "    \"\"\"\n",
    "    - User gets the corresponding model and callback list based on the type he set during instance creation of this class\n",
    "    \"\"\"\n",
    "    def getModel(self):\n",
    "        if self.modelType == \"cnn-rnn\":\n",
    "            return (self.__get_rnn_model(), self.__get_rnn_callback())\n",
    "        elif self.modelType == \"conv3d\" or self.modelType == \"OF\":\n",
    "            return (self.__get_conv3d_model(), self.__get_conv3d_callback())\n",
    "        elif self.modelType == \"QuoVadis\":\n",
    "            pass\n",
    "        else:\n",
    "            print(\"ERROR: Should not come here, please set modelType to cnn-rnn, conv3d, OF or QuoVadis\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"asparagus\"> $\\Rightarrow$ Initialize Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeSeeds():\n",
    "    np.random.seed(30)\n",
    "    rn.seed(30)\n",
    "    tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='skyblue'>  III.2. Let's try various models </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializeSeeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"asparagus\"> $\\Rightarrow$ Some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameIdxList = list(range(0, 30))\n",
    "width = 224\n",
    "height = 224\n",
    "numChannels = 3\n",
    "numClasses = 5\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "val_path = r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\val\"\n",
    "val_csv = r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\val.csv\"\n",
    "numFeaturesInFirstLayer = 16\n",
    "numNeuronsInDenseLayer = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.1 Inception + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = \"cnn-rnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDeployment = ModelDeployment(input_shape=(width, height, numChannels), modelType=modelType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generators\n",
    "train_gen = CNNRNNGenerator(frameIdxList=frameIdxList, batch_size=batch_size)\n",
    "train_gen.initializeFeatureExtractor()\n",
    "\n",
    "val_gen = CNNRNNGenerator(frameIdxList=frameIdxList, source_path=val_path, batch_size=batch_size,\n",
    "                          dataCSV=val_csv)\n",
    "val_gen.initializeFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks_list = modelDeployment.getModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "model.fit(train_gen,\n",
    "          steps_per_epoch=train_gen.getNumBatches(),\n",
    "          epochs=num_epochs,\n",
    "          verbose=1,\n",
    "          validation_data=val_gen,\n",
    "          validation_steps=val_gen.getNumBatches(),\n",
    "          workers=6,\n",
    "          use_multiprocessing=False,\n",
    "          initial_epoch=0,\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.2 Custom Conv3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convTrainGen = Conv3DGenerator(frameIdxList=frameIdxList, batch_size=batch_size)\n",
    "convValGen = Conv3DGenerator(frameIdxList=frameIdxList, source_path=val_path, batch_size=batch_size,\n",
    "                             dataCSV=val_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = \"conv3d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "convModelDeployment = ModelDeployment(input_shape=(width, height, numChannels), modelType=modelType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks_list = convModelDeployment.getModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.3 Optical flow followed by a custom conv3d network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = \"OF\"\n",
    "ofModelDeployment = ModelDeployment(input_shape=(width, height, 2), modelType=modelType,numFrames=(len(frameIdxList) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainOFGen = OpticalFlowGenerator(frameIdxList=frameIdxList, batch_size=batch_size)\n",
    "valOFGen = OpticalFlowGenerator(frameIdxList=frameIdxList, source_path=val_path, batch_size=batch_size,\n",
    "                                dataCSV=val_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks_list = ofModelDeployment.getModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.4 Quo Vadis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"asparagus\"> $\\Rightarrow$ Load the two best models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ofModel = models.load_model(\n",
    "            r\"D:\\PyCharm\\Projects\\GestureRecognition\\conv3d_of\\conv3d_init_2023-01-0213_04_57.758259\\conv3d-00005-0.12532-0.96866-0.27041-0.93000.h5\")\n",
    "\n",
    "conv3dModel = models.load_model(\n",
    "    r\"D:\\PyCharm\\Projects\\GestureRecognition\\conv3d\\conv3d_init_2023-01-0101_51_17.081632\\conv3d-00017-0.07573-0.97761-0.68541-0.88000.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get val generator for Conv3d\n",
    "convValGen = Conv3DGenerator(frameIdxList=frameIdxList, source_path=val_path, batch_size=batch_size,\n",
    "                             dataCSV=val_csv)\n",
    "# Get val generator for OF-Conv3d. Use the same data_doc as above\n",
    "valOFGen = OpticalFlowGenerator(frameIdxList=frameIdxList, source_path=val_path, batch_size=batch_size,\n",
    "                                dataCSV=val_csv, useVectorList=True, vectorList=convValGen.vectorList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"asparagus\"> $\\Rightarrow$ Logic for consolidating the results of the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuoVadisAccuracy(convValGen, valOFGen):\n",
    "    noOfEqual = 0\n",
    "    totalNoOfIters = (valOFGen.numVideos / valOFGen.batch_size)\n",
    "    iter = 0\n",
    "    for ((convBatchData, convBatchLabels), (ofBatchData, ofBatchLabels)) in zip(convValGen, valOFGen):\n",
    "        # Conv3D model's pred\n",
    "        conv3dPreds = conv3dModel.predict(convBatchData)\n",
    "\n",
    "        # OF model's pred\n",
    "        ofModelPreds = ofModel.predict(ofBatchData)\n",
    "\n",
    "        # Averaging the two\n",
    "        final_predProbas = (ofModelPreds + conv3dPreds) / 2\n",
    "\n",
    "        # Obtaining predictions with argmax\n",
    "        final_pred = np.argmax(final_predProbas, axis=1)  # Max across each row\n",
    "\n",
    "        # Obtain acutal label\n",
    "        actualLabel = np.argmax(convBatchLabels, axis=1)\n",
    "\n",
    "        # For calculating accuracy:\n",
    "        noOfEqual += sum([1 for pred, act in zip(final_pred, actualLabel) if pred == act])\n",
    "        iter += 1\n",
    "        if iter >= totalNoOfIters: break\n",
    "    return ((noOfEqual/valOFGen.numVideos) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getQuoVadisAccuracy(convValGen=convValGen, valOFGen=valOFGen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
