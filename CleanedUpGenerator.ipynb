{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "To build a 3D Conv model that will be able to predict the 5 gestures correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='cyan'> Sections in this notebook: </font>\n",
    "I. Prerequisites\n",
    "    \n",
    "    I.1. Importing all the necessary modules\n",
    "    I.2. Shuffle the data\n",
    "    \n",
    "II. Custom Generator\n",
    "\n",
    "III. Model Deployment\n",
    "\n",
    "    III.1. Custom Conv3d Model\n",
    "    III.2. Train and Val Generators\n",
    "    III.3. Few more setup related steps \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='goldenrod'> I. Prerequisites </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='skyblue'>  I.1. Importing all the necessary modules </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from cv2 import imread, resize\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rn\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> We set the random seed so that the results don't vary drastically. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "rn.seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='skyblue'>  I.2. Shuffle the data </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Read all the lines in the csv and randomly permute them. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"red \"> TODO: REMOVE THIS COMMENT !! </font> <br>In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\train\"\n",
    "val_path = r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCSV = r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\train.csv\"\n",
    "valCSV = r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\val.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open(trainCSV).readlines())\n",
    "val_doc = np.random.permutation(open(valCSV).readlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n"
     ]
    }
   ],
   "source": [
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Set the batch size. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"red \"> TODO: REMOVE THIS SECTION... Used for debugging cv2 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = os.listdir(train_path + \"\\\\\" + train_doc[0].split(';')[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WIN_20180926_16_54_08_Pro_00006.png'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = imread(train_path + \"\\\\\" + train_doc[0].split(';')[0] + \"\\\\\" + img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 160)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[0], x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_img = resize(x, dim, interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv2.imshow(\"img\", resized_img)\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv2.imshow(\"Orig\", x)\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b ,g, r = cv2.split(resized_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"red \"> TODO: Up to this part</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='goldenrod'> II. Custom Generator </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Class Names/Labels:  </font> <br>\n",
    "- Left to Right : 0 <br>\n",
    "- Right to Left: 1<br>\n",
    "- Stop: 2<br>\n",
    "- Thumbs down: 3<br>\n",
    "- Thums up: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    width = None \n",
    "    height = None \n",
    "    numChannels = 3\n",
    "    \n",
    "    source_path = None\n",
    "    vectorList = None\n",
    "    batch_size = None\n",
    "    frameIdxList = None\n",
    "    numFramesInVideo = None\n",
    "    numVideso = None\n",
    "    def __init__(self,\n",
    "                 folder_list,\n",
    "                 imgIdxList,\n",
    "                 width=224,\n",
    "                 height=224,\n",
    "                 source_path=r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\train\",\n",
    "                 batch_size=75):\n",
    "        self.vectorList = np.random.permutation(folder_list) # Shuffle the data and store in a list\n",
    "        #print(self.vectorList)\n",
    "        self.frameIdxList = imgIdxList\n",
    "        self.numFramesInVideo = len(imgIdxList)\n",
    "        self.numVideos = len(folder_list)\n",
    "        self.source_path = source_path\n",
    "        self.batch_size = batch_size\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.numOfBatches = self.numVideos // self.batch_size\n",
    "        \n",
    "    # Loop through current batch size --> get one folder at a time -->\n",
    "    # loop through each image in a folder --> preprocess --> One hot encode the label --> yield\n",
    "    def __getBatchData(self, batch, curr_batch_size):\n",
    "        batch_data = np.zeros((batch_size, self.numFramesInVideo, \n",
    "                               self.width, self.height, self.numChannels)) \n",
    "        # batch_labels is the one hot representation of the output\n",
    "        batch_labels = np.zeros((batch_size, 5))\n",
    "        for folderIdx in range(curr_batch_size):\n",
    "             # Get vector/folder name\n",
    "            ## Turn this on for debugging\n",
    "            #print(folderIdx + (batch*batch_size))\n",
    "            vectorName = self.vectorList[folderIdx + (batch*self.batch_size)].strip().split(';')[0]\n",
    "            #print(vectorName)\n",
    "            imgs = os.listdir(self.source_path+'/'+ vectorName)\n",
    "            # Iterate iver the frames/images of a folder to read them in\n",
    "            for idx,item in enumerate(self.frameIdxList):\n",
    "                # Get the image in float32 \n",
    "                image = imread(self.source_path+'/'+ vectorName +'/'+imgs[item]).astype(np.float32)\n",
    "                # Resize\n",
    "                resized_img = resize(image, (self.width, self.height), interpolation = cv2.INTER_AREA)\n",
    "                # Normalize\n",
    "                resized_img = resized_img / 255.0\n",
    "                #crop the images ## TO DO, we are resizing for now\n",
    "                channels = cv2.split(resized_img) # b g r\n",
    "                batch_data[folderIdx,idx,:,:,0] = channels[0]\n",
    "                batch_data[folderIdx,idx,:,:,1] = channels[1]\n",
    "                batch_data[folderIdx,idx,:,:,2] = channels[2]\n",
    "            # One hot encoding\n",
    "            batch_labels[folderIdx, int(self.vectorList[folderIdx + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "    # Public method, call this to get generator object\n",
    "    def generator(self):\n",
    "        while True:\n",
    "            for batch in range(self.numOfBatches):\n",
    "                batch_data, batch_labels = self.__getBatchData(batch, self.batch_size)\n",
    "                yield batch_data, batch_labels\n",
    "            # For the remaining data points which are left after full batches\n",
    "            batch += 1\n",
    "            rem_batch_size = self.numVideos % self.batch_size\n",
    "            batch_data, batch_labels = self.__getBatchData(batch, rem_batch_size)\n",
    "            yield batch_data, batch_labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"red \"> TODO: REMOVE THIS COMMENT !! </font> <br> Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Some global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgIdxList = list(range(0,30))\n",
    "# (width, height) is the final size of the input images \n",
    "# numChannels = 3 (RGB)\n",
    "width = 224\n",
    "height = 224\n",
    "numChannels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> For testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['WIN_20180926_16_54_08_Pro_Right_Swipe_new;Right_Swipe_new;1\\n',\n",
       "       'WIN_20180925_18_02_58_Pro_Thumbs_Down_new;Thumbs_Down_new;3\\n',\n",
       "       'WIN_20180925_17_33_08_Pro_Left_Swipe_new;Left_Swipe_new;0\\n',\n",
       "       'WIN_20180925_17_51_17_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n'],\n",
       "      dtype='<U88')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_doc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(folder_list=train_doc, \n",
    "                      imgIdxList=imgIdxList, \n",
    "                      width=width, \n",
    "                      height=height, \n",
    "                      source_path=train_path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = gen.generator() # Create generator Class' instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data, batch_label =  next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 30, 224, 224, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data.shape # VideoIdx, FrameIdxInVideo, width, height, numChannels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 224, 224, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Change the first index to 0, 1, 2, .., (batch_size -1) to view the image. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv2.imshow(\"First\", batch_data[2][0])\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data, batch_label =  next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv2.imshow(\"First\", batch_data[0][0])\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> If you try to see batch_data[1][0], it should be all zeros, since batch size is 3, number of videos = 4. The second next(train_generator) statement will generate only one video, the other two will tensors will be all zeros because of: <br> batch_data = np.zeros((batch_size, self.numFramesInVideo, self.width, self.height, self.numChannels)) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='goldenrod'> III. Model Deployment </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"red \"> TODO: REMOVE THIS COMMENT !! </font> <br> \n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output layer parameters\n",
    "n_output          =  5 # number of classes in case of classification, 1 in case of regression\n",
    "output_activation =  \"softmax\"# \"softmax\" or \"sigmoid\" in case of classification, \"linear\" in case of regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeaturesInFirstLayer = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='skyblue'>  III.1. Custom Conv3d model </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Utility to get conv3d model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv3d_model(numFeaturesInFirstLayer, numFrames, output_activation,\n",
    "                     width=224, height=224, numChannels=3,\n",
    "                     numClasses=5, numNeuronsInDenseLayer=256):\n",
    "    #Model\n",
    "    model=models.Sequential()\n",
    "\n",
    "    # Convolution layer with 64 features, 3x3 filter and relu activation with 2x2 pooling\n",
    "    model.add(layers.Conv3D(numFeaturesInFirstLayer,(3,3,3),padding = 'same',activation='relu', \n",
    "                            input_shape=(numFrames,width,height,numChannels)))\n",
    "    model.add(layers.MaxPooling3D())\n",
    "\n",
    "    # Convolution layer with 128 features, 3x3 filter and relu activation with 2x2 pooling\n",
    "    model.add(layers.Conv3D((numFeaturesInFirstLayer*2),(3,3,3),padding = 'same',activation='relu'))\n",
    "    model.add(layers.MaxPooling3D())\n",
    "\n",
    "    # Convolution layer with 128 features, 3x3 filter and relu activation with 2x2 pooling\n",
    "    model.add(layers.Conv3D((numFeaturesInFirstLayer*4),(3,3,3),padding = 'same',activation='relu'))\n",
    "    model.add(layers.MaxPooling3D())\n",
    "    \n",
    "    # Convolution layer with 128 features, 3x3 filter and relu activation with 2x2 pooling\n",
    "    model.add(layers.Conv3D((numFeaturesInFirstLayer*8),(3,3,3),padding = 'same',activation='relu'))\n",
    "    model.add(layers.MaxPooling3D())\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(numNeuronsInDenseLayer,activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(layers.Dense(numClasses,activation=output_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Some Constants </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeaturesInFirstLayer = 16\n",
    "numFrames = len(imgIdxList)\n",
    "model = get_conv3d_model(numFeaturesInFirstLayer, numFrames, output_activation=output_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Compile model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 30, 224, 224, 16)  1312      \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 15, 112, 112, 16)  0        \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 15, 112, 112, 32)  13856     \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 7, 56, 56, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 7, 56, 56, 64)     55360     \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 3, 28, 28, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 3, 28, 28, 128)    221312    \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 1, 14, 14, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               6422784   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,715,909\n",
      "Trainable params: 6,715,909\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = \"adam\"\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='skyblue'>  III.2. Train and Val Generators </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(folder_list=train_doc, \n",
    "                      imgIdxList=imgIdxList, \n",
    "                      width=width, \n",
    "                      height=height, \n",
    "                      source_path=train_path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = gen.generator() # Get generator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen_obj = Generator(folder_list=val_doc,\n",
    "                        imgIdxList=imgIdxList, \n",
    "                        width=width, \n",
    "                        height=height, \n",
    "                        source_path=val_path, \n",
    "                        batch_size=batch_size)\n",
    "val_generator = val_gen_obj.generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='skyblue'>  III.3. Few more setup related steps </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Creating some callbacks</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dt_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.3.1. Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santo\\AppData\\Local\\Temp\\ipykernel_25392\\385748685.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5848 - categorical_accuracy: 0.2313\n",
      "Epoch 1: saving model to model_init_2022-12-2800_21_56.575700\\model-00001-1.58484-0.23134-1.40353-0.48000.h5\n",
      "67/67 [==============================] - 132s 2s/step - loss: 1.5848 - categorical_accuracy: 0.2313 - val_loss: 1.4035 - val_categorical_accuracy: 0.4800 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.2194 - categorical_accuracy: 0.4776\n",
      "Epoch 2: saving model to model_init_2022-12-2800_21_56.575700\\model-00002-1.21942-0.47761-0.95839-0.65000.h5\n",
      "67/67 [==============================] - 63s 954ms/step - loss: 1.2194 - categorical_accuracy: 0.4776 - val_loss: 0.9584 - val_categorical_accuracy: 0.6500 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.8428 - categorical_accuracy: 0.6164\n",
      "Epoch 3: saving model to model_init_2022-12-2800_21_56.575700\\model-00003-0.84276-0.61642-0.79920-0.71000.h5\n",
      "67/67 [==============================] - 63s 945ms/step - loss: 0.8428 - categorical_accuracy: 0.6164 - val_loss: 0.7992 - val_categorical_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.6391 - categorical_accuracy: 0.7418\n",
      "Epoch 4: saving model to model_init_2022-12-2800_21_56.575700\\model-00004-0.63910-0.74179-0.67916-0.82000.h5\n",
      "67/67 [==============================] - 63s 944ms/step - loss: 0.6391 - categorical_accuracy: 0.7418 - val_loss: 0.6792 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4902 - categorical_accuracy: 0.8254\n",
      "Epoch 5: saving model to model_init_2022-12-2800_21_56.575700\\model-00005-0.49023-0.82537-0.71483-0.81000.h5\n",
      "67/67 [==============================] - 63s 941ms/step - loss: 0.4902 - categorical_accuracy: 0.8254 - val_loss: 0.7148 - val_categorical_accuracy: 0.8100 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.3851 - categorical_accuracy: 0.8448\n",
      "Epoch 6: saving model to model_init_2022-12-2800_21_56.575700\\model-00006-0.38508-0.84478-0.57779-0.80000.h5\n",
      "67/67 [==============================] - 64s 962ms/step - loss: 0.3851 - categorical_accuracy: 0.8448 - val_loss: 0.5778 - val_categorical_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.2625 - categorical_accuracy: 0.9119\n",
      "Epoch 7: saving model to model_init_2022-12-2800_21_56.575700\\model-00007-0.26253-0.91194-0.73061-0.77000.h5\n",
      "67/67 [==============================] - 60s 898ms/step - loss: 0.2625 - categorical_accuracy: 0.9119 - val_loss: 0.7306 - val_categorical_accuracy: 0.7700 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.2026 - categorical_accuracy: 0.9179\n",
      "Epoch 8: saving model to model_init_2022-12-2800_21_56.575700\\model-00008-0.20263-0.91791-0.66000-0.81000.h5\n",
      "67/67 [==============================] - 60s 896ms/step - loss: 0.2026 - categorical_accuracy: 0.9179 - val_loss: 0.6600 - val_categorical_accuracy: 0.8100 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.2131 - categorical_accuracy: 0.9313\n",
      "Epoch 9: saving model to model_init_2022-12-2800_21_56.575700\\model-00009-0.21312-0.93134-0.74099-0.82000.h5\n",
      "67/67 [==============================] - 60s 905ms/step - loss: 0.2131 - categorical_accuracy: 0.9313 - val_loss: 0.7410 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.2652 - categorical_accuracy: 0.9209\n",
      "Epoch 10: saving model to model_init_2022-12-2800_21_56.575700\\model-00010-0.26516-0.92090-0.64053-0.80000.h5\n",
      "67/67 [==============================] - 60s 897ms/step - loss: 0.2652 - categorical_accuracy: 0.9209 - val_loss: 0.6405 - val_categorical_accuracy: 0.8000 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c29ea079a0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(train_generator, \n",
    "          steps_per_epoch=steps_per_epoch, \n",
    "          epochs=num_epochs, \n",
    "          verbose=1,\n",
    "          callbacks=callbacks_list, \n",
    "          validation_data=val_generator,\n",
    "          validation_steps=validation_steps, \n",
    "          class_weight=None, \n",
    "          workers=1, initial_epoch=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"red\"> TODO!! Plot history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='skyblue'>  III.4. CNN + RNN </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.4.1. Feature Extraction: CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Base Model: VGG16 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Model\n",
    "# I don't want the fully connected layers on the right; I'll add my own: include_top = False\n",
    "baseModel = (VGG16(weights=\"imagenet\", include_top=False, input_shape=(width, height, numChannels)))\n",
    "baseModel.trainable = False ## Don't start training all over again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Add dense layers and ouput layer with 5 classes </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattenLayer = layers.Flatten()\n",
    "denseLayer = layers.Dense(256, activation=\"relu\")\n",
    "denseLayer = layers.Dense(512, activation=\"relu\")\n",
    "outputLayer = layers.Dense(5, activation=\"softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Train and Val Generator objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(folder_list=train_doc, \n",
    "                      imgIdxList=imgIdxList, \n",
    "                      width=width, \n",
    "                      height=height, \n",
    "                      source_path=train_path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = gen.generator() # Get train generator object\n",
    "val_generator = val_gen_obj.generator() # Get val generator obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = \"adam\"\n",
    "baseModel.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ImageDataGenerator object\n",
    "datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19890 images belonging to 663 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create the generator object using the flow_from_directory method\n",
    "generator = datagen.flow_from_directory(train_path,\n",
    "                                        target_size=(224, 224),\n",
    "                                        batch_size=batch_size,\n",
    "                                        class_mode=None,  # Set class_mode to None since we are not using labels\n",
    "                                        shuffle=False)  # Set shuffle to False to preserve the order of the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow(\"img\", out[2])\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    cv2.imshow(\"img\", out[i])\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Model.predict_generator() got an unexpected keyword argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mbaseModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Model.predict_generator() got an unexpected keyword argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "features = baseModel.predict_generator(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not an iterator"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
