{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "To build a 3D Conv model that will be able to predict the 5 gestures correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='cyan'> Sections in this notebook: </font>\n",
    "I. Prerequisites\n",
    "    \n",
    "    I.1. Importing all the necessary modules\n",
    "    I.2. Shuffle the data\n",
    "    \n",
    "II. Custom Generator\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='goldenrod'> I. Prerequisites </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='skyblue'>  I.1. Importing all the necessary modules </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from cv2 import imread, resize\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rn\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> We set the random seed so that the results don't vary drastically. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "rn.seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='skyblue'>  I.2. Shuffle the data </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Read all the lines in the csv and randomly permute them. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"red \"> TODO: REMOVE THIS COMMENT !! </font> <br>In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\train\"\n",
    "val_path = r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCSV = r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\train.csv\"\n",
    "valCSV = r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\val.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open(trainCSV).readlines())\n",
    "val_doc = np.random.permutation(open(valCSV).readlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n"
     ]
    }
   ],
   "source": [
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Set the batch size. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"red \"> TODO: REMOVE THIS SECTION... Used for debugging cv2 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = os.listdir(train_path + \"\\\\\" + train_doc[0].split(';')[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WIN_20180926_16_54_08_Pro_00006.png'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = imread(train_path + \"\\\\\" + train_doc[0].split(';')[0] + \"\\\\\" + img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 160)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[0], x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_img = resize(x, dim, interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow(\"img\", resized_img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow(\"Orig\", x)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "b ,g, r = cv2.split(resized_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"red \"> TODO: Up to this part</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='goldenrod'> II. Custom Generator </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Class Names/Labels:  </font> <br>\n",
    "- Left to Right : 0 <br>\n",
    "- Right to Left: 1<br>\n",
    "- Stop: 2<br>\n",
    "- Thumbs down: 3<br>\n",
    "- Thums up: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    width = None \n",
    "    height = None \n",
    "    numChannels = 3\n",
    "    \n",
    "    source_path = None\n",
    "    vectorList = None\n",
    "    batch_size = None\n",
    "    frameIdxList = None\n",
    "    numFramesInVideo = None\n",
    "    numVideso = None\n",
    "    def __init__(self,\n",
    "                 folder_list,\n",
    "                 imgIdxList,\n",
    "                 width=224,\n",
    "                 height=224,\n",
    "                 source_path=r\"D:\\DDownloads\\UpGrad\\NeuralNetwork\\CaseStudy\\Project_data\\train\",\n",
    "                 batch_size=75):\n",
    "        self.vectorList = np.random.permutation(folder_list) # Shuffle the data and store in a list\n",
    "        print(self.vectorList)\n",
    "        self.frameIdxList = imgIdxList\n",
    "        self.numFramesInVideo = len(imgIdxList)\n",
    "        self.numVideos = len(folder_list)\n",
    "        self.source_path = source_path\n",
    "        self.batch_size = batch_size\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.numOfBatches = self.numVideos // self.batch_size\n",
    "        \n",
    "    # Loop through current batch size --> get one folder at a time -->\n",
    "    # loop through each image in a folder --> preprocess --> One hot encode the label --> yield\n",
    "    def __getBatchData(self, batch, curr_batch_size):\n",
    "        batch_data = np.zeros((batch_size, self.numFramesInVideo, \n",
    "                               self.width, self.height, self.numChannels)) \n",
    "        # batch_labels is the one hot representation of the output\n",
    "        batch_labels = np.zeros((batch_size, 5))\n",
    "        for folderIdx in range(curr_batch_size):\n",
    "             # Get vector/folder name\n",
    "            ## Turn this on for debugging\n",
    "            #print(folderIdx + (batch*batch_size))\n",
    "            vectorName = self.vectorList[folderIdx + (batch*self.batch_size)].strip().split(';')[0]\n",
    "            #print(vectorName)\n",
    "            imgs = os.listdir(self.source_path+'/'+ vectorName)\n",
    "            # Iterate iver the frames/images of a folder to read them in\n",
    "            for idx,item in enumerate(self.frameIdxList):\n",
    "                # Get the image in float32 \n",
    "                image = imread(self.source_path+'/'+ vectorName +'/'+imgs[item]).astype(np.float32)\n",
    "                # Resize\n",
    "                resized_img = resize(image, (self.width, self.height), interpolation = cv2.INTER_AREA)\n",
    "                # Normalize\n",
    "                resized_img = resized_img / 255.0\n",
    "                #crop the images ## TO DO, we are resizing for now\n",
    "                channels = cv2.split(resized_img) # b g r\n",
    "                batch_data[folderIdx,idx,:,:,0] = channels[0]\n",
    "                batch_data[folderIdx,idx,:,:,1] = channels[1]\n",
    "                batch_data[folderIdx,idx,:,:,2] = channels[2]\n",
    "            # One hot encoding\n",
    "            batch_labels[folderIdx, int(self.vectorList[folderIdx + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "    # Public method, call this to get generator object\n",
    "    def generator(self):\n",
    "        while True:\n",
    "            for batch in range(self.numOfBatches):\n",
    "                batch_data, batch_labels = self.__getBatchData(batch, self.batch_size)\n",
    "                yield batch_data, batch_labels\n",
    "            # For the remaining data points which are left after full batches\n",
    "            batch += 1\n",
    "            rem_batch_size = self.numVideos % self.batch_size\n",
    "            batch_data, batch_labels = self.__getBatchData(batch, rem_batch_size)\n",
    "            yield batch_data, batch_labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"red \"> TODO: REMOVE THIS COMMENT !! </font> <br> Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Some global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgIdxList = list(range(0,30,2))\n",
    "batch_size = 75\n",
    "# (width, height) is the final size of the input images \n",
    "# numChannels = 3 (RGB)\n",
    "width = 224\n",
    "height = 224\n",
    "numChannels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> For testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['WIN_20180926_16_54_08_Pro_Right_Swipe_new;Right_Swipe_new;1\\n',\n",
       "       'WIN_20180925_18_02_58_Pro_Thumbs_Down_new;Thumbs_Down_new;3\\n',\n",
       "       'WIN_20180925_17_33_08_Pro_Left_Swipe_new;Left_Swipe_new;0\\n',\n",
       "       'WIN_20180925_17_51_17_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n'],\n",
       "      dtype='<U88')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_doc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WIN_20180925_18_02_58_Pro_Thumbs_Down_new;Thumbs_Down_new;3\\n'\n",
      " 'WIN_20180925_17_51_17_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n'\n",
      " 'WIN_20180925_17_33_08_Pro_Left_Swipe_new;Left_Swipe_new;0\\n'\n",
      " 'WIN_20180926_16_54_08_Pro_Right_Swipe_new;Right_Swipe_new;1\\n']\n"
     ]
    }
   ],
   "source": [
    "gen = Generator(folder_list=train_doc[:4], \n",
    "                      imgIdxList=imgIdxList, \n",
    "                      width=width, \n",
    "                      height=height, \n",
    "                      source_path=train_path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = gen.generator() # Create generator Class' instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data, batch_label =  next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 15, 224, 224, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data.shape # VideoIdx, FrameIdxInVideo, width, height, numChannels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 224, 224, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> Change the first index to 0, 1, 2, .., (batch_size -1) to view the image. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow(\"First\", batch_data[2][0])\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data, batch_label =  next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow(\"First\", batch_data[0][0])\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ <font color=\"asparagus\"> If you try to see batch_data[1][0], it should be all zeros, since batch size is 3, number of videos = 4. The second next(train_generator) statement will generate only one video, the other two will tensors will be all zeros because of: <br> batch_data = np.zeros((batch_size, self.numFramesInVideo, self.width, self.height, self.numChannels)) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "#write your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimiser = #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dt_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
